D:\programs\Anaconda\envs\pytorch\python.exe G:/笔记本数据保存/Y700数据互通/软件安装/chatgpt_academic-master/1.py
ResidualAlexNet(
  (conv1): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): PReLU(num_parameters=1)
  (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): PReLU(num_parameters=1)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): PReLU(num_parameters=1)
    (6): Linear(in_features=4096, out_features=10, bias=True)
  )
)
(tensor([[ 0.0410, -0.0192, -0.3142, -0.3567,  0.8141,  0.4100,  0.0217, -0.4752,
          0.5587, -0.8292]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.0943, 0.0888, 0.0661, 0.0634, 0.2043, 0.1364, 0.0925, 0.0563, 0.1583,
         0.0395]], device='cuda:0', grad_fn=<SoftmaxBackward0>))
Epoch: 001/010 | Batch: 000/938 | Cost: 2.4198
Epoch: 001/010 | Batch: 120/938 | Cost: 1.5961
Epoch: 001/010 | Batch: 240/938 | Cost: 0.2532
Epoch: 001/010 | Batch: 360/938 | Cost: 0.2557
Epoch: 001/010 | Batch: 480/938 | Cost: 1.5116
Epoch: 001/010 | Batch: 600/938 | Cost: 0.3773
Epoch: 001/010 | Batch: 720/938 | Cost: 0.3220
Epoch: 001/010 | Batch: 840/938 | Cost: 0.0735
Epoch: 001/010 | Train Acc.: 96.56% | Validation Acc.: 96.67%
Time elapsed: 0.66 min
Epoch: 002/010 | Batch: 000/938 | Cost: 0.1595
Epoch: 002/010 | Batch: 120/938 | Cost: 0.3353
Epoch: 002/010 | Batch: 240/938 | Cost: 0.5021
Epoch: 002/010 | Batch: 360/938 | Cost: 0.4260
Epoch: 002/010 | Batch: 480/938 | Cost: 0.1483
Epoch: 002/010 | Batch: 600/938 | Cost: 0.0179
Epoch: 002/010 | Batch: 720/938 | Cost: 0.0204
Epoch: 002/010 | Batch: 840/938 | Cost: 0.0631
Epoch: 002/010 | Train Acc.: 97.91% | Validation Acc.: 97.94%
Time elapsed: 1.31 min
Epoch: 003/010 | Batch: 000/938 | Cost: 0.0883
Epoch: 003/010 | Batch: 120/938 | Cost: 0.2741
Epoch: 003/010 | Batch: 240/938 | Cost: 0.0322
Epoch: 003/010 | Batch: 360/938 | Cost: 0.4444
Epoch: 003/010 | Batch: 480/938 | Cost: 0.6859
Epoch: 003/010 | Batch: 600/938 | Cost: 0.1353
Epoch: 003/010 | Batch: 720/938 | Cost: 0.2752
Epoch: 003/010 | Batch: 840/938 | Cost: 0.1488
Epoch: 003/010 | Train Acc.: 97.64% | Validation Acc.: 97.78%
Time elapsed: 1.96 min
Epoch: 004/010 | Batch: 000/938 | Cost: 0.0511
Epoch: 004/010 | Batch: 120/938 | Cost: 0.0070
Epoch: 004/010 | Batch: 240/938 | Cost: 0.0201
Epoch: 004/010 | Batch: 360/938 | Cost: 0.0452
Epoch: 004/010 | Batch: 480/938 | Cost: 0.1868
Epoch: 004/010 | Batch: 600/938 | Cost: 0.0693
Epoch: 004/010 | Batch: 720/938 | Cost: 0.3714
Epoch: 004/010 | Batch: 840/938 | Cost: 0.1469
Epoch: 004/010 | Train Acc.: 98.09% | Validation Acc.: 98.09%
Time elapsed: 2.61 min
Epoch: 005/010 | Batch: 000/938 | Cost: 0.0455
Epoch: 005/010 | Batch: 120/938 | Cost: 0.0011
Epoch: 005/010 | Batch: 240/938 | Cost: 0.0485
Epoch: 005/010 | Batch: 360/938 | Cost: 0.0411
Epoch: 005/010 | Batch: 480/938 | Cost: 0.0859
Epoch: 005/010 | Batch: 600/938 | Cost: 0.0130
Epoch: 005/010 | Batch: 720/938 | Cost: 0.0123
Epoch: 005/010 | Batch: 840/938 | Cost: 0.0960
Epoch: 005/010 | Train Acc.: 99.07% | Validation Acc.: 98.78%
Time elapsed: 3.26 min
Epoch: 006/010 | Batch: 000/938 | Cost: 0.0097
Epoch: 006/010 | Batch: 120/938 | Cost: 0.0565
Epoch: 006/010 | Batch: 240/938 | Cost: 0.1442
Epoch: 006/010 | Batch: 360/938 | Cost: 0.1272
Epoch: 006/010 | Batch: 480/938 | Cost: 0.1838
Epoch: 006/010 | Batch: 600/938 | Cost: 0.0179
Epoch: 006/010 | Batch: 720/938 | Cost: 0.1060
Epoch: 006/010 | Batch: 840/938 | Cost: 0.0162
Epoch: 006/010 | Train Acc.: 99.13% | Validation Acc.: 98.93%
Time elapsed: 3.91 min
Epoch: 007/010 | Batch: 000/938 | Cost: 0.0079
Epoch: 007/010 | Batch: 120/938 | Cost: 0.0821
Epoch: 007/010 | Batch: 240/938 | Cost: 0.0176
Epoch: 007/010 | Batch: 360/938 | Cost: 0.0084
Epoch: 007/010 | Batch: 480/938 | Cost: 0.0052
Epoch: 007/010 | Batch: 600/938 | Cost: 0.1192
Epoch: 007/010 | Batch: 720/938 | Cost: 0.0692
Epoch: 007/010 | Batch: 840/938 | Cost: 0.0010
Epoch: 007/010 | Train Acc.: 98.65% | Validation Acc.: 98.24%
Time elapsed: 4.56 min
Epoch: 008/010 | Batch: 000/938 | Cost: 0.0424
Epoch: 008/010 | Batch: 120/938 | Cost: 0.0517
Epoch: 008/010 | Batch: 240/938 | Cost: 0.0237
Epoch: 008/010 | Batch: 360/938 | Cost: 0.1376
Epoch: 008/010 | Batch: 480/938 | Cost: 3.1440
Epoch: 008/010 | Batch: 600/938 | Cost: 0.6712
Epoch: 008/010 | Batch: 720/938 | Cost: 0.5410
Epoch: 008/010 | Batch: 840/938 | Cost: 0.4817
Epoch: 008/010 | Train Acc.: 92.56% | Validation Acc.: 92.64%
Time elapsed: 5.21 min
Epoch: 009/010 | Batch: 000/938 | Cost: 0.2788
Epoch: 009/010 | Batch: 120/938 | Cost: 0.6735
Epoch: 009/010 | Batch: 240/938 | Cost: 0.3681
Epoch: 009/010 | Batch: 360/938 | Cost: 0.2930
Epoch: 009/010 | Batch: 480/938 | Cost: 0.2738
Epoch: 009/010 | Batch: 600/938 | Cost: 0.1907
Epoch: 009/010 | Batch: 720/938 | Cost: 0.1971
Epoch: 009/010 | Batch: 840/938 | Cost: 0.1772
Epoch: 009/010 | Train Acc.: 96.67% | Validation Acc.: 96.62%
Time elapsed: 5.86 min
Epoch: 010/010 | Batch: 000/938 | Cost: 0.1357
Epoch: 010/010 | Batch: 120/938 | Cost: 0.0893
Epoch: 010/010 | Batch: 240/938 | Cost: 0.1927
Epoch: 010/010 | Batch: 360/938 | Cost: 0.2072
Epoch: 010/010 | Batch: 480/938 | Cost: 0.1996
Epoch: 010/010 | Batch: 600/938 | Cost: 0.0244
Epoch: 010/010 | Batch: 720/938 | Cost: 0.0669
Epoch: 010/010 | Batch: 840/938 | Cost: 0.0641
Epoch: 010/010 | Train Acc.: 97.57% | Validation Acc.: 97.65%
Time elapsed: 6.51 min
Total Training Time: 6.51 min
